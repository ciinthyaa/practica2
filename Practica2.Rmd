---
title: "Practica2"
author: "Cinthya Figueroa, David Vidal y Valeri Suarez"
date: "2024-01-16"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
library(stringr)
library(readr)

## Datos Elegantes + Análisis de Datos con Web Scrapping

#### Pregunta 1


Queremos programar un programa de tipo web scrapping con el que podamos obtener una página web, mediante su URL, y poder analizar su contenido HTML con tal de extraer datos e información específica.
Nuestro programa ha de ser capaz de cumplir con los siguientes pasos:

1. Descargar la página web de la URL indicada, y almacenarlo en un formato de R apto para ser tratado.

read_html
 
2. Analizar el contenido de la web, buscando el título de la página (que en HTML se etiqueta como “title”).


 
3. Analizar el contenido de la web, buscando todos los enlaces (que en HTML se etiquetan como “a”), buscando el texto del enlace, así como la URL.


 
4. Generar una tabla con cada enlace encontrado, indicando el texto que acompaña el enlace, y el número de veces que aparece un enlace con ese mismo objetivo.
 

  
5. Para cada enlace, seguirlo e indicar si está activo (podemos usar el código de status HTTP al hacer una petición a esa URL).


 
### Pregunta 2

Elaborad, usando las librerías de gráficos base y qplot (ggplot2), una infografía sobre los datos obtenidos. Tal infografía será una reunión de gráficos donde se muestren los siguientes detalles:

1. Un histograma con la frecuencia de aparición de los enlaces, pero separado por URLs absolutas (con “http…”) y URLs relativas.


 
2. Un gráfico de barras indicando la suma de enlaces que apuntan a otros dominios o servicios (distinto a https://www.mediawiki.org en el caso de ejemplo) vs. la suma de los otros enlaces.


 
3. Un gráfico de tarta (pie chart) indicando los porcentajes de Status de nuestro análisis.


 
## Análisis de logs de servidor usando R (parte II)

### Obtención y carga de los Datos:

Queremos programar un script con el que podamos hacer una investigación forense sobre un fichero de logs de un servidor de tipo Apache. Los datos del registro del servidor están en el formato estándar e incluyen miles de registros sobre las distintas peticiones gestionadas por el servidor web.

Nuestro programa ha de ser capaz de obtener las respuestas de forma dinámica a las siguientes preguntas utilizando instrucciones de código en R:

1. Descomprimir el fichero comprimido que contiene los registros del servidor, y a partir de los datos extraídos, cargar en data frame los registros con las peticiones servidas.

Damos por hecho que el fichero comprimido está guardado en C:\temp con nombre epa_http.zip y descomprimirá el contenido en la misma carpeta c:\temp, teniendo el fichero epa-http.csv en la misma ruta.

```{r}

zipF<- "C:\\temp\\epa_http.zip"
outDir<-"C:\\temp"
unzip(zipF,exdir=outDir)

epa_http <- read_table ("c:/temp/epa-http.csv", col_names = FALSE, col_types = cols(X7 = col_number()))
View(epa_http)
```
2. Incluid en el documento un apartado con la descripción de los datos analizados: fuente, tipología, descripción de la información contenida (los diferentes campos) y sus valores.

```{r}
colnames (epa_http) <- c("IPs", "Timestamp", "Tipo", "URL", "Protocolo", "Código de respuesta", "Bytes")
epa_http$IPs<- as.factor(epa_http$IPs)
epa_http$Tipo <- str_replace (epa_http$Tipo, "\"", "")
epa_http$Tipo<- as.factor(epa_http$Tipo)
epa_http$URL<- as.factor(epa_http$URL)
epa_http$Protocolo <- str_replace (epa_http$Protocolo, "\"", "")
epa_http$Protocolo<- as.factor(epa_http$Protocolo)
epa_http$`Código de respuesta` <- as.factor(epa_http$`Código de respuesta`)
summary (epa_http)
```
 
Descripción campo IPs: IP de acceso al servidor host
Descripción campo Timestamp: Fecha y zona horaria de la petición específica
Descripción campo Tipo: Método invocado
Descripción campo URL: URL solicitada
Descripción campo Protocolo: Protocolo utilizado
Descripción campo Código de respuesta: Resultados de código
Descripción campo Bytes: Número de bytes transferidos
 
### Limpieza de los Datos

3. Aprovechando que los datos a analizar son los mismos de la primera práctica, para esta entrega es imprescindible que los datos estén en formato de “datos elegantes”.


 
### Exploración de Datos

4. Identificar el número único de usuarios que han interactuado directamente con el servidor de forma segregada según si los usuarios han tenido algún tipo de error en las distintas peticiones ofrecidas por el servidor.


 
### Análisis de Datos

5. Analizar los distintos tipos de peticiones HTTP (GET, POST, PUT, DELETE) gestionadas por el servidor, identificando la frecuencia de cada una de estas. Repetir el análisis, esta vez filtrando previamente aquellas peticiones correspondientes a recursos ofrecidos de tipo imagen.


 
### Visualización de Resultados

6. Generar al menos 2 gráficos distintos que permitan visualizar alguna característica relevante de los datos analizados.

Estos deberán representar por lo menos 1 o 2 variables diferentes del data frame. Describid el gráfico e indicad cualquier observación destacable que se pueda apreciar gracias a la representación gráfica.


 
7. Generar un gráfico que permita visualizar el número de peticiones servidas a lo largo del tiempo.


 
### Clústering de datos

8. Utilizando un algoritmo de aprendizaje no supervisado, realizad un análisis de clústering con k-means para los datos del servidor.


 
9. Representad visualmente en gráficos de tipo scatter plot el resultado de vuestros clústering y interpretad el resultado obtenido


 
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
