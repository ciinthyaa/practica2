---
title: "Práctica 2"
author: "Cinthya Figueroa, David Vidal y Valeri Suarez"
date: "2024-01-16"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r}

library(stringr)
library(dplyr)
library(ggplot2)
library(readr)
library(rvest)
library(XML)
library(httr)
library(httr2)
library(htmlTable)
library(patchwork)
```

## Datos Elegantes + Análisis de Datos con Web Scrapping

#### Pregunta 1


Queremos programar un programa de tipo web scrapping con el que podamos obtener una página web, mediante su URL, y poder analizar su contenido HTML con tal de extraer datos e información específica.
Nuestro programa ha de ser capaz de cumplir con los siguientes pasos:

1. Descargar la página web de la URL indicada, y almacenarlo en un formato de R apto para ser tratado.
```{r}
    url <- "https://www.mediawiki.org/wiki/MediaWiki"
    response <- GET(url)
    writeBin(response$content, "wikipedia.html")
```

 
2. Analizar el contenido de la web, buscando el título de la página (que en HTML se etiqueta como “title”).

```{r}
    pxml <- htmlParse("wikipedia.html")
    title <- xpathSApply(pxml, "//title", xmlValue)
    cat("El título es:", title, "\n")
```
3. Analizar el contenido de la web, buscando todos los enlaces (que en HTML se etiquetan como “a”), buscando el texto del enlace, así como la URL.

```{r}
    enlaces <- xpathSApply(pxml,"//a", function(x) c(URL= xmlGetAttr(x, "href"), Texto = xmlValue(x)))
    val_nulos <- sapply(enlaces, is.null)
    enlaces[val_nulos] <- NA
    V_lista <- unlist (enlaces)
```

4. Generar una tabla con cada enlace encontrado, indicando el texto que acompaña el enlace, y el número de veces que aparece un enlace con ese mismo objetivo.

    Se crea la tabla:
    
```{r}
    tabla <- data.frame(URL = enlaces[seq(1, length(enlaces), 2)], Texto = enlaces[seq(2, length(enlaces), 2)])
    tabla_repetidos <- table(tabla$URL)
    tabla$conteo <- tabla_repetidos[match(tabla$URL, names(tabla_repetidos))]
```
    
    Se realiza el conteo de los enlaces y se muestra la cantidad de veces que se repite cada enlace, sin agrupar:

```{r}
    tabla_repetidos <- table(tabla$URL)
    tabla$conteo <- tabla_repetidos[match(tabla$URL, names(tabla_repetidos))]
    tabla
``` 
5. Para cada enlace, seguirlo e indicar si está activo (podemos usar el código de status HTTP al hacer una petición a esa URL).

    Primero se crea una función que busque las URLS que inicien con "https://"  y en caso de no tener esa cadena de caracteres pegue la URL del dominio:

```{r fsdgd}
burls<- function(url) {
  ifelse(grepl("https://", url), url, paste("https://www.mediawiki.org", url, sep = ''))
                      }
``` 

    En la tabla se crea una nueva columna que almacenará los resultados de la funcion creada anteriormente:
    
```{r}
    tabla$linksr <- sapply (tabla$URL, burls)
``` 
  
    Con las URLS completas se procede a verificar el estado de cada link:
    
```{r}
    tabla$estado <- sapply( tabla$linksr, function(x) {res <- GET (x)
    Sys.sleep(1)
      status_code (res)} )
    tabla
``` 

### Pregunta 2

Elaborad, usando las librerías de gráficos base y qplot (ggplot2), una infografía sobre los datos obtenidos. Tal infografía será una reunión de gráficos donde se muestren los siguientes detalles:

1. Un histograma con la frecuencia de aparición de los enlaces, pero separado por URLs absolutas (con “http…”) y URLs relativas.

  Se crea una nueva columna llamada 'Tipo' para segun la condición clasificar entre URLs absolutas y relativas
```{r}
tabla$Tipo <- ifelse(grepl("https:", tabla$URL), "URL Absoluta", "URL Relativa")
``` 

Con los datos obtenidos se grafica el histograma

```{r}
gra1 <- ggplot(data = tabla, aes(x = Tipo, fill = Tipo)) + geom_bar() + labs(title = "Histograma de tipo de URLs", x = "Tipo de URL", y = "Frecuencia") + theme_minimal()
gra1
``` 

2. Un gráfico de barras indicando la suma de enlaces que apuntan a otros dominios o servicios (distinto a https://www.mediawiki.org en el caso de ejemplo) vs. la suma de los otros enlaces.

Creamos una nueva columna llamada 'TipoEnlace2' para guardar la diferencia entre enlaces a mediawiki y otros:

```{r}
tabla$TipoEnlace2 <- ifelse( grepl("https://www.mediawiki.org", tabla$linksr), "MediaWiki", "OTRA URL"
 )
``` 

Creamos el gráfico de barras:

```{r}
gra2 <- ggplot(data = tabla, aes(x = TipoEnlace2, fill = TipoEnlace2)) +
     geom_bar() +
     labs(title = "Mediawiki vs otras URLs", x = "Tipo de URL", y = "Cantidad de URLs") + theme_minimal()
gra2
``` 
3. Un gráfico de tarta (pie chart) indicando los porcentajes de Status de nuestro análisis.

Se obtiene el porcentaje de cada estado y luego se grafica el pie chart

```{r}
porc<- paste0(round(prop.table(table(tabla$estado)) * 100))

gra3 <-  ggplot(data.frame(porc), aes(x="", y= porc, fill= porc))+ geom_bar(stat = "identity", width = 1) +  coord_polar(theta = "y") + geom_text(aes(label = paste0(porc, "%")), position = position_stack(vjust = 0.5))
gra3
``` 

Se pueden visualizar los tres graficos juntos:
```{r}
gra1 + gra2 + gra3
```

Otra forma puede ser, guardar 2 graficas en una variable y luego guardar esta más el ultimo grafico en un variable total con la división de vista que queremos: 

 
## Análisis de logs de servidor usando R (parte II)

### Obtención y carga de los Datos:

Queremos programar un script con el que podamos hacer una investigación forense sobre un fichero de logs de un servidor de tipo Apache. Los datos del registro del servidor están en el formato estándar e incluyen miles de registros sobre las distintas peticiones gestionadas por el servidor web.

Nuestro programa ha de ser capaz de obtener las respuestas de forma dinámica a las siguientes preguntas utilizando instrucciones de código en R:

1. Descomprimir el fichero comprimido que contiene los registros del servidor, y a partir de los datos extraídos, cargar en data frame los registros con las peticiones servidas.
Damos por hecho que el fichero comprimido está guardado en C:\temp con nombre epa_http.zip y descomprimirá el contenido en la misma carpeta c:\temp, teniendo el fichero epa-http.csv en la misma ruta.

```{r}

zipF<- "C:\\temp\\epa_http.zip"
outDir<-"C:\\temp"
unzip(zipF,exdir=outDir)

epa_http <- read_table ("c:/temp/epa-http.csv", col_names = FALSE, col_types = cols(X7 = col_number()))
View(epa_http)
```
2. Incluid en el documento un apartado con la descripción de los datos analizados: fuente, tipología, descripción de la información contenida (los diferentes campos) y sus valores.

```{r}
colnames (epa_http) <- c("IPs", "Timestamp", "Tipo", "URL", "Protocolo", "Código de respuesta", "Bytes")
epa_http$IPs<- as.factor(epa_http$IPs)
epa_http$Tipo <- str_replace (epa_http$Tipo, "\"", "")
epa_http$Tipo<- as.factor(epa_http$Tipo)
epa_http$URL<- as.factor(epa_http$URL)
epa_http$Protocolo <- str_replace (epa_http$Protocolo, "\"", "")
epa_http$Protocolo<- as.factor(epa_http$Protocolo)
epa_http$`Código de respuesta` <- as.factor(epa_http$`Código de respuesta`)
summary (epa_http)
```
 
Descripción campo IPs: IP de acceso al servidor host
Descripción campo Timestamp: Fecha y zona horaria de la petición específica
Descripción campo Tipo: Método invocado
Descripción campo URL: URL solicitada
Descripción campo Protocolo: Protocolo utilizado
Descripción campo Código de respuesta: Resultados de código
Descripción campo Bytes: Número de bytes transferidos
 
### Limpieza de los Datos

3. Aprovechando que los datos a analizar son los mismos de la primera práctica, para esta entrega es imprescindible que los datos estén en formato de “datos elegantes”.

```{r}
epa_http$Timestamp <- as.POSIXct(epa_http$Timestamp, format="[%d:%H:%M:%S]", tz="UTC")
epa_http
```

### Exploración de Datos

4. Identificar el número único de usuarios que han interactuado directamente con el servidor de forma segregada según si los usuarios han tenido algún tipo de error en las distintas peticiones ofrecidas por el servidor.

```{r}
# Creamos una variable lógica para los errores (los códigos de respuesta 4xx y 5xx)
epa_http$error <- grepl("^[45]", epa_http$`Código de respuesta`)

# Agrupamos los datos por IP y error y mostramos resumen
epa_http %>% 
  group_by(IPs, error) %>% 
  summarize(count = n_distinct(Tipo))

# Pasamos a numérico el código de error para poder hacer comparaciones numérias
epa_http$`Código de respuesta` <- as.numeric(as.character(epa_http$`Código de respuesta`))

# Crear una variable para el tipo de error
epa_http$tipo_error <- case_when(
  epa_http$`Código de respuesta` >= 500 ~ "Error del servidor",
  epa_http$`Código de respuesta` >= 400 ~ "Error del cliente",
  TRUE ~ "Sin error"
)

# Creamos una tabla con la agrupación de la respuesta según si es error del cliente, servidor o si no hay error
tabla_frecuencias <- table(epa_http$tipo_error)

# Creamos una tabla con la frecuencia de los distintos tipos de códigos de respuesta
tabla_tipos_respuesta <- table(epa_http$`Código de respuesta`)

# Mostramos los datos de las dos tablas anteriores
print(tabla_frecuencias)
print(tabla_tipos_respuesta)

# Ahora agrupamos los datos por IP y tipo de error y mostramos resumen
epa_http %>% 
  group_by(IPs, tipo_error) %>% 
  summarize(count = n_distinct(IPs))

```
Descripción error 400 (Bad Request): Es un error general que indica que tu navegador envía una solicitud al servidor del sitio web y el servidor no puede procesar o reconocer la solicitud.
Descripción error 403 (Forbidden): No tienes permiso para ver todo o parte del sitio web por alguna razón.
Descripción error 404 (Page Not Found): El sitio web no existe, puede que hahya sido movido o borrado.
Descripción error 500 (Internal Server Error): Error interno del servidor.
Descripción error 501 (Not Implemented): El servidor no soporta la funcionalidad necesaria para satisfacer la solicitud.


### Análisis de Datos

5. Analizar los distintos tipos de peticiones HTTP (GET, POST, PUT, DELETE) gestionadas por el servidor, identificando la frecuencia de cada una de estas. Repetir el análisis, esta vez filtrando previamente aquellas peticiones correspondientes a recursos ofrecidos de tipo imagen.

```{r}
# Agrupamos por método HTTP y mostramos resumen de los datos 
epa_http %>% 
  group_by(Tipo) %>% 
  summarize(Total = n())

# Creamos un gráfico de barras del número de peticiones por método HTTP
ggplot(epa_http, aes(x = Tipo)) +
  geom_bar() +
  labs(title = "Número de peticiones por método HTTP", x = "Método", y = "Peticiones")


# Repetimos análisis esta vez filtrando previamente por peticiones de imágenes
epa_http$imagen <- grepl("\\.(jpg|png|gif|bmp|svg)$", epa_http$URL, ignore.case = TRUE)

# Filtramos el log para que muestre solo las filas que tienen el campo imagen que creamos a TRUE
epa_http_imagenes <- filter(epa_http, imagen == TRUE)

# Agrupamos por método HTTP y mostramos resumen de los datos filtrados por peticiones de imágenes
epa_http_imagenes %>% 
  group_by(Tipo) %>% 
  summarize(Total = n())

# Creamos un gráfico de barras del número de peticiones por método HTTP filtradas por peticiones de imágenes
ggplot(epa_http_imagenes, aes(x = Tipo)) +
  geom_bar() +
  labs(title = "Número de peticiones por método HTTP", x = "Método", y = "Peticiones")
```

### Visualización de Resultados

6. Generar al menos 2 gráficos distintos que permitan visualizar alguna característica relevante de los datos analizados.

Estos deberán representar por lo menos 1 o 2 variables diferentes del data frame. Describid el gráfico e indicad cualquier observación destacable que se pueda apreciar gracias a la representación gráfica.

```{r}
# Creamos una tabla solo con la columna con los códigos de respuesta del log
pie_frecuencias <- table(epa_http$`Código de respuesta`)

# Generamos un gráfico de sectores con la función pie
pie(pie_frecuencias, labels = names(pie_frecuencias))

# Se evidencia en el gráfico que más de un 75% de las peticiones son servidas correctamente por el servidor

# Generamos un gráfico de barras de los datos por IP y tipos de error con la columan tipo_error que creamos en el ejercicio 4
ggplot(epa_http, aes(x = IPs, fill = tipo_error)) +
     geom_bar(stat = "count", position = "stack") +
     labs(title = "Número de usuarios únicos por IP y tipo de error", x = "IP", y = "Usuarios", fill = "Tipo de error")

# Se evidencia en el gráfico que hay una IP que tiene más del doble de peticiones servidas correctamente que el resto

```

7. Generar un gráfico que permita visualizar el número de peticiones servidas a lo largo del tiempo.

Como ya hicimos limpieza de datos en el punto 3 y tenemos la hora y fecha con formato Date de R podemos generar directamente el gráfico.

```{r}
# Generamos un gráfico de líneas con el número de peticiones por fecha y hora
ggplot(epa_http, aes(x = Timestamp)) +
  geom_line(stat = "count") +
  labs(title = "Número de peticiones servidas a lo largo del tiempo", x = "Fecha y hora", y = "Peticiones")
```

### Clústering de datos

8. Utilizando un algoritmo de aprendizaje no supervisado, realizad un análisis de clústering con k-means para los datos del servidor.

```{r}
 
```

9. Representad visualmente en gráficos de tipo scatter plot el resultado de vuestros clústering y interpretad el resultado obtenido

```{r}

```
